# -*- coding: utf-8 -*-
"""unsloth.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SoFmDYY_a2Iw9n8MVFyD1DI1D0iZdxkd

##**FINETUNING Llama3 with custom Dataset using unsloth**##
Fine-tuning LLaMA3 with a custom dataset of Docker commands using Unsloth to enable the model to generate Docker commands based on user input. This fine-tuning will enhance the model's capability to provide accurate and relevant Docker commands
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# #install Unsloth ,Xformers (Flash Attention and all other packages)
# !pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# !pip install --no-deps "xformers<0.0.27" "trl<0.9.0" peft accelerate bitsandbytes

from unsloth import FastLanguageModel
import torch
max_sequence_length = 2048
dtype = None
load_in_4bit = True

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name= "unsloth/Meta-Llama-3.1-8B",
    max_seq_length = max_sequence_length,
    dtype =dtype,
    load_in_4bit= load_in_4bit,
)

model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
    ],

    lora_alpha=16,
    lora_dropout=0,
    bias ="none",
        use_gradient_checkpointing = "unsloth",
    random_state = 3047,
    use_rslora = False,
    loftq_config = None,

)

"""###logging into hugging face###"""

from huggingface_hub import notebook_login

notebook_login()

"""##Dataset Preparation##
Dataset Overview: Describe the dataset used for fine-tuning, including the types of inputs and expected outputs.
"""

from datasets import load_dataset
#Load the dataset
dataset = load_dataset("MattCoddity/dockerNLcommands",split='train')

# Print the column names and the length of each column
print("Columns and their lengths:")
for column in dataset.column_names:
    print(f"Column: {column}, Length: {len(dataset[column])}")

def formatting_prompt_func(example):
    return {
        "input": example["input"],
        "instruction": example["instruction"],
        "output": example["output"]
    }

dataset = dataset.map(formatting_prompt_func)

print("Dataset Features:",dataset.features)

formatted_dataset = dataset.map(formatting_prompt_func, batched=True)

"""##Prompt design###"""

docker_prompt = """below is an instruction describes a task , paired with an input that provides further context, write a response that approves the instruction .

### Instruction:
{}

### Input:
{}

### Response:
{}"""


EOS_Token= tokenizer.eos_token
def formatting_prompt_func(examples):
  instructions = examples["instruction"]
  inputs       = examples["input"]
  outputs       = examples["output"]
  texts = []
  for intructions,  input, output in zip(instructions,inputs,outputs):

    text = docker_prompt.format(instructions,input,output)+EOS_Token
    texts.append(text)
    return {"texts":texts,}
pass

"""##Model Fine-Tuning##"""

from trl import  SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bf16_supported


trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    dataset_text_field="input",
    tokenizer=tokenizer,
    max_seq_length=max_sequence_length,
    packing=False,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=5,
        max_steps=50,
        learning_rate=2e-4,
        fp16=not is_bf16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        lr_scheduler_type="linear",
        seed=3407,
        output_dir="outputs",
    )
)

trainer_stats = trainer.train()

FastLanguageModel.for_inference(model)

input = tokenizer(
 [

    docker_prompt.format(
      "translate this sentence in docker command.", #instruction
      "Give me a list of containers that have the Ubuntu image as their ancestor.", #input
      "", #output
  )
], return_tensors="pt").to("cuda")

output = model.generate(**input,max_new_tokens=64, use_cache = True)
tokenizer.batch_decode(output)

"""##Gradio Inteface##"""

!pip install gradio

import gradio as gr


docker_prompt = """below is an instruction describes a task , paired with an input that provides further context, write a response that approves the instruction .

### Instruction:
{}

### Input:
{}

### Response:
{}"""

def generate_docker_command(user_input):
    formatted_input = docker_prompt.format(
        instruction,
        input,
        ""  # output (empty, as we want the model to generate this)
    )

    inputs = tokenizer(
        [formatted_input],
        return_tensors="pt"
    ).to("cuda")

    outputs = model.generate(
        **inputs,
        max_new_tokens=64,
        use_cache=True
    )

    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    return decoded_outputs[0]


interface = gr.Interface(
    fn=generate_docker_command,
    inputs=gr.Textbox(label="Input", placeholder="Enter the text you want to convert to a Docker command."),
    outputs=gr.Textbox(label="Docker Command"),
    title="Docker Command Generator",
    description="Enter input text to generate a Docker command using a fine-tuned model."
)


interface.launch()



